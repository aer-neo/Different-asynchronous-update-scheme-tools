{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# In the working directory, you must upload your rules in a .txt file.\n",
        "\n",
        "Example format:\n",
        "\n",
        "A: C\n",
        "B: C\n",
        "C: A or B\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0mFQQhfVaYI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different asynchronous update scheme tools"
      ],
      "metadata": {
        "id": "45dPjRaZwACw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_yhlBWMaJ4r",
        "outputId": "8b68dd00-8f75-434d-a96d-703600075422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defined nodes: ['A', 'B', 'C']\n",
            "Edges (dependencies):\n",
            "  C -> A\n",
            "  C -> B\n",
            "  A -> C\n",
            "  B -> C\n",
            "\n",
            "Total number of update schedules (block-sequential): 13\n",
            "Number of update classes (based on update digraph): 9\n",
            "\n",
            "Representative schedule of each class:\n",
            "\n",
            "Scheme_1; [1, 1, 1]\n",
            "Scheme_2; [1, 1, 2]\n",
            "Scheme_3; [1, 2, 1]\n",
            "Scheme_4; [1, 2, 2]\n",
            "Scheme_5; [2, 1, 1]\n",
            "Scheme_6; [2, 1, 2]\n",
            "Scheme_7; [2, 2, 1]\n",
            "Scheme_8; [1, 3, 2]\n",
            "Scheme_9; [3, 1, 2]\n",
            "\n",
            "Results exported to update_schedules.csv\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import re\n",
        "import csv\n",
        "\n",
        "def parse_rules_from_file(filename=\"rules.txt\"):\n",
        "    \"\"\"\n",
        "    Reads node definitions from a text file.\n",
        "    Format: node: Boolean expression.\n",
        "    Returns:\n",
        "      - functions: dict {node: expression}\n",
        "      - dependencies: dict {node: set of dependent nodes}\n",
        "    \"\"\"\n",
        "    functions = {}\n",
        "    dependencies = {}\n",
        "\n",
        "    token_pattern = r'\\b[\\w]+\\b'\n",
        "    keywords = {\"and\", \"or\", \"not\", \"True\", \"False\"}\n",
        "\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                node, expr = line.split(\":\", 1)\n",
        "            except ValueError:\n",
        "                print(f\"Incorrect format in line: {line}\")\n",
        "                continue\n",
        "            node = node.strip()\n",
        "            expr = expr.strip()\n",
        "            functions[node] = expr\n",
        "            tokens = re.findall(token_pattern, expr)\n",
        "            deps = {token for token in tokens if token not in keywords and token != node}\n",
        "            dependencies[node] = deps\n",
        "\n",
        "    return functions, dependencies\n",
        "\n",
        "def build_interaction_graph(dependencies):\n",
        "    edges = []\n",
        "    for target, deps in dependencies.items():\n",
        "        for source in deps:\n",
        "            if source in dependencies:\n",
        "                edges.append((source, target))\n",
        "    return edges\n",
        "\n",
        "def compute_signature(schedule, edges):\n",
        "    \"\"\"\n",
        "    Given an update schedule (a dict {node: block}) and the list of edges (source, target),\n",
        "    computes the signature of the update digraph.\n",
        "\n",
        "    For each edge (j, i):\n",
        "      - If schedule[j] >= schedule[i], label as \"P\"\n",
        "      - If schedule[j] < schedule[i], label as \"N\"\n",
        "\n",
        "    Returns a sorted tuple of (j, i, label)\n",
        "    \"\"\"\n",
        "    signature = []\n",
        "    for j, i in edges:\n",
        "        label = \"P\" if schedule[j] >= schedule[i] else \"N\"\n",
        "        signature.append((j, i, label))\n",
        "    return tuple(sorted(signature))\n",
        "\n",
        "def generate_block_schedules(nodes):\n",
        "    \"\"\"\n",
        "    Generates all block-sequential update schedules for the nodes.\n",
        "    An update schedule is a surjective function s: nodes -> {1, ..., m} (for m from 1 to len(nodes)).\n",
        "    Returns a list of schedules (each is a dict {node: block}).\n",
        "    \"\"\"\n",
        "    schedules = []\n",
        "    n = len(nodes)\n",
        "    for m in range(1, n + 1):\n",
        "        for values in itertools.product(range(1, m + 1), repeat=n):\n",
        "            if set(values) == set(range(1, m + 1)):  # ensure surjectivity\n",
        "                sched = {node: value for node, value in zip(nodes, values)}\n",
        "                schedules.append(sched)\n",
        "    return schedules\n",
        "\n",
        "def group_update_schedules(nodes, edges):\n",
        "    \"\"\"\n",
        "    Generates all block-sequential update schedules and groups them by equivalence class (signature).\n",
        "    Keeps the first schedule that appears in each class.\n",
        "    Returns a dictionary: {signature: list of schedules}.\n",
        "    \"\"\"\n",
        "    groups = {}\n",
        "    schedules = generate_block_schedules(nodes)\n",
        "    for sched in schedules:\n",
        "        signature = compute_signature(sched, edges)\n",
        "        if signature not in groups:\n",
        "            groups[signature] = []\n",
        "        groups[signature].append(sched)\n",
        "    return groups\n",
        "\n",
        "def schedule_to_list(schedule, nodes):\n",
        "    \"\"\"\n",
        "    Converts a schedule (dict {node: block}) into a list of integers\n",
        "    according to the order of the 'nodes' list.\n",
        "    \"\"\"\n",
        "    return [schedule[node] for node in nodes]\n",
        "\n",
        "def export_to_csv(results, filename=\"update_schedules.csv\"):\n",
        "    \"\"\"\n",
        "    Exports the result (list of tuples: (scheme_name, schedule_list)) to a CSV file.\n",
        "    \"\"\"\n",
        "    with open(filename, mode=\"w\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Scheme\", \"Schedule\"])\n",
        "        for scheme_name, schedule_list in results:\n",
        "            writer.writerow([scheme_name, schedule_list])\n",
        "    print(f\"\\nResults exported to {filename}\")\n",
        "\n",
        "def main():\n",
        "    functions, dependencies = parse_rules_from_file(\"rules.txt\")\n",
        "    nodes = list(functions.keys())\n",
        "\n",
        "    print(\"\\nDefined nodes:\", nodes)\n",
        "\n",
        "    edges = build_interaction_graph(dependencies)\n",
        "    print(\"Edges (dependencies):\")\n",
        "    for j, i in edges:\n",
        "        print(f\"  {j} -> {i}\")\n",
        "\n",
        "    groups = group_update_schedules(nodes, edges)\n",
        "    total_schedules = len(generate_block_schedules(nodes))\n",
        "    print(f\"\\nTotal number of update schedules (block-sequential): {total_schedules}\")\n",
        "    print(f\"Number of update classes (based on update digraph): {len(groups)}\\n\")\n",
        "\n",
        "    print(\"Representative schedule of each class:\\n\")\n",
        "    results = []\n",
        "    for idx, (signature, schedules) in enumerate(groups.items(), start=1):\n",
        "        representative = schedules[0]\n",
        "        order_list = schedule_to_list(representative, nodes)\n",
        "        scheme_name = f\"Scheme_{idx}\"\n",
        "        print(f\"{scheme_name}; {order_list}\")\n",
        "        results.append((scheme_name, order_list))\n",
        "\n",
        "    export_to_csv(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# For 9 nodes it takes approximately 16 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining basins of attraction for each asynchronous update scheme"
      ],
      "metadata": {
        "id": "KohWRzvIwHNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "\n",
        "def load_rules_from_file(filename=\"rules.txt\"):\n",
        "    \"\"\"\n",
        "    Loads Boolean rules from a text file.\n",
        "    Returns a dictionary {node: expression}\n",
        "    \"\"\"\n",
        "    rules = {}\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                node, expr = line.split(\":\", 1)\n",
        "                rules[node.strip()] = expr.strip()\n",
        "            except ValueError:\n",
        "                print(f\"Incorrect format in line: {line}\")\n",
        "    return rules\n",
        "\n",
        "def simulate_network(rules, update_vector, initial_state):\n",
        "    state_transition_table = []\n",
        "    for i in range(2 ** len(initial_state)):\n",
        "        state_str = format(i, f\"0{len(initial_state)}b\")\n",
        "        current_state = {node: int(state_str[idx]) for idx, node in enumerate(initial_state)}\n",
        "        steps = set(update_vector)\n",
        "        for step in steps:\n",
        "            next_state = current_state.copy()\n",
        "            nodes_to_update = [node for idx, node in enumerate(rules) if update_vector[idx] == step]\n",
        "            for node in nodes_to_update:\n",
        "                local_env = {key: bool(val) for key, val in current_state.items()}\n",
        "                next_state[node] = eval(rules[node], {}, local_env)\n",
        "            current_state = next_state.copy()\n",
        "        result_state_str = ''.join(str(int(current_state[node])) for node in sorted(current_state))\n",
        "        state_transition_table.append(f\"{state_str} => {result_state_str}\")\n",
        "    return state_transition_table\n",
        "\n",
        "def normalize_cycle(cycle):\n",
        "    min_index = min(range(len(cycle)), key=lambda i: cycle[i])\n",
        "    return cycle[min_index:] + cycle[:min_index]\n",
        "\n",
        "def find_attractors(transition_table):\n",
        "    state_map = {state.split(' => ')[0]: state.split(' => ')[1] for state in transition_table}\n",
        "    attractors = {}\n",
        "    basin_map = {}\n",
        "    for state in state_map.keys():\n",
        "        visited = []\n",
        "        current_state = state\n",
        "        trajectory = []\n",
        "        while current_state not in visited:\n",
        "            visited.append(current_state)\n",
        "            trajectory.append(current_state)\n",
        "            current_state = state_map[current_state]\n",
        "        if current_state in visited:\n",
        "            cycle_start = visited.index(current_state)\n",
        "            cycle = tuple(visited[cycle_start:])\n",
        "            normalized_cycle = normalize_cycle(cycle)\n",
        "            if normalized_cycle not in attractors:\n",
        "                attractors[normalized_cycle] = set()\n",
        "            for s in trajectory:\n",
        "                basin_map[s] = normalized_cycle\n",
        "            attractors[normalized_cycle].update(visited)\n",
        "    return attractors, basin_map\n",
        "\n",
        "# Load rules from file\n",
        "rules = load_rules_from_file(\"rules.txt\")\n",
        "\n",
        "# Initialize all nodes to 0\n",
        "initial_state = {node: 0 for node in sorted(rules.keys())}\n",
        "\n",
        "# Load update vectors with comma separator\n",
        "file_path = './update_schedules.csv'\n",
        "update_vectors_df = pd.read_csv(file_path, sep=',', header=0)\n",
        "\n",
        "# Confirm column names\n",
        "print(\"üìã CSV column names:\", update_vectors_df.columns.tolist())\n",
        "\n",
        "schedule_column = \"Schedule\"\n",
        "\n",
        "results = []\n",
        "for index, row in update_vectors_df.iterrows():\n",
        "    try:\n",
        "        update_vector = ast.literal_eval(row[schedule_column])\n",
        "        if len(update_vector) != len(rules):\n",
        "            print(f\"‚ö†Ô∏è Skipping row {index}: expected {len(rules)} elements, got {len(update_vector)}\")\n",
        "            continue\n",
        "\n",
        "        transition_table = simulate_network(rules, update_vector, initial_state)\n",
        "        attractors, basin_map = find_attractors(transition_table)\n",
        "\n",
        "        for attractor, basin in sorted(attractors.items(), key=lambda item: len(item[1]), reverse=True):\n",
        "            results.append({\n",
        "                \"Update Vector\": str(update_vector),\n",
        "                \"Attractor\": attractor,\n",
        "                \"Basin Size\": len(basin)\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing row {index}: {e}\")\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('./Attractor_Results.csv', index=False, sep=';')\n",
        "print(\"‚úÖ Results saved to './Attractor_Results.csv'\")\n",
        "\n",
        "# For 11000 Scheme it takes approximately 11 minutes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcTFBjt3eF3T",
        "outputId": "3eb6385f-6e2e-444d-f23a-f9d33177dcbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã CSV column names: ['Scheme', 'Schedule']\n",
            "‚úÖ Results saved to './Attractor_Results.csv'\n"
          ]
        }
      ]
    }
  ]
}